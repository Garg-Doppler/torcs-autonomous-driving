%
% Example for a student report latex file. Adapt as necessary
%

% the following lines should stay as is
\documentclass[10pt,a4paper,twoside,journal]{IEEEtran}
\usepackage[nocompress]{cite}
\usepackage[pdftex]{graphicx}

% some packages that most people use or improve the result
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsbsy}
\usepackage{flushend}
% properly print units, enable compact product between units
\usepackage[inter-unit-product =\cdot]{siunitx}
% load units \bit, \byte usw
\sisetup{detect-weight=true, binary-units=true}
\DeclareSIUnit\px{px}
\usepackage{array}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{url}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{pgf}

\usepackage[switch]{lineno}
\makeatletter
\@ifpackageloaded{lineno}{%
	\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
	  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
	  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
	  \renewenvironment{#1}%
		 {\linenomath\csname old#1\endcsname}%
		 {\csname oldend#1\endcsname\endlinenomath}}%
	\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
	  \patchAmsMathEnvironmentForLineno{#1}%
	  \patchAmsMathEnvironmentForLineno{#1*}}%
	\AtBeginDocument{%
	\patchBothAmsMathEnvironmentsForLineno{equation}%
	\patchBothAmsMathEnvironmentsForLineno{align}%
	\patchBothAmsMathEnvironmentsForLineno{flalign}%
	\patchBothAmsMathEnvironmentsForLineno{alignat}%
	\patchBothAmsMathEnvironmentsForLineno{gather}%
	\patchBothAmsMathEnvironmentsForLineno{multline}%
	}}%
	{}
\makeatother

% add your additional packages here
\usepackage{lipsum}

\begin{document}

%
% When submitting the review, make sure to include the following line to enable
% line numbering. When submitting the final report, disable the following line!
\linenumbers
%

%
% configure submission details
%

% here you can specify the day of submission
\newcommand{\submissiondate}{\today}

% please specify the type of your submission. E.g. Advanced Seminar or Practical
% Laboratory
\newcommand{\submissiontype}{Practical course: Computational Neuro Engineering}

% give information about when your course happened in form of SEMESTER YEAR,
% e.g. Winter Semester 2016. In addition, specify the "short version", e.g. WS
% 2016
\newcommand{\submissionterm}{Winter Semester 2017/2018}
\newcommand{\submissiontermshort}{WS 2017/2018}

% the full submission title
\newcommand{\submissiontitle}{Learning to drive based on multiple sensor cues
in The Open Racing Car Simulator (TORCS)}
% in case that you have a very long report title, make sure to provide a shorter
% version that can be used in the \markboth command further below. In case your
% topic is short, simply comment the next and uncommented the second line
\newcommand{\submissiontitleshort}{Autonomous driving in TORCS with multiple sensors}
%\newcommand{\submissiontitleshort}{\submissiontitle}

% author list. Make sure that you include your matriculation number in the
% section starting with \thanks. In addition, specify your supervisor(s).
\author{Tim~Bicker and Nimar~Blume%
	\thanks{\textbf{Authors}:
		Tim Bicker (12345678, tim.bicker@tum.de)
		and Nimar Blume (03638934, nimar.blume@tum.de)
		% the following includes additional information
		\textbf{Course}: \submissiontype{} \submissionterm{}
		\textbf{Submitted}: \submissiondate{}
		\textbf{Supervisor}: Florian Mirus.
		Neuroscientific System Theory (Prof. Dr. J\"org Conradt), Technische
		Universit\"at M\"unchen, Arcisstra√üe 21, 80333 M\"unchen, Germany.
}}

% both items should look alike and contain a short version of the type of work
% and semester (e.g. Advanced Seminar WS 2016, Project Laboratory SS 2017) and
% your report title. If your title is too long, find a shorter one.
\markboth{\submissiontype{} \submissiontermshort{}: \submissiontitleshort{}}
{\submissiontype{} \submissiontermshort{}: \submissiontitleshort{}}

% this generates the paper title
\title{\submissiontitle}
\maketitle

% write a short abstract to introduce the reader to your work
\begin{abstract}
	To implement an autonomous driver in The Open Racing Car Simulator (TORCS)
	based on a deep neural network (DNN) and spiking neural network (SNN) multiple
	sensor cues are used. Specifically, the DNN predicts the current car displacement
	and angle to the road centre from a driver's view image. Based on the two values
	a SNN generates driving commands for the car. Subsequently, the car is put onto a 
	new track and the driving performance is evaluated.\
	The DNN is based on a Convolutional Neural Network and after training the mean 
	absolute error for the displacement is XXXX and for the angle is XXX on an
	unseen test track.
\end{abstract}

\begin{IEEEkeywords}
	deep learning, TORCS, convolutional neural network, spiking neural network, autonomous driving
\end{IEEEkeywords}

%
% Main body follows here. Only capitalize the first word in any title
%
\section{Introduction}
\label{sc:intro}

\IEEEPARstart{A}{utonomous} driving is currently the subject of much controversy, especially
its feasibility and performance. TODO: INSERT MORE BACKGROUND INFO! 
In this paper The Open Racing Car Simulator (TORCS) is used
to simulate a car on a race track and provide relevant sensor data. The goal is to use multiple sensor cues to drive the car safely around the defined test tracks. Through the Robot Operating System (ROS) the following sensors can be read from TORCS: 
\begin{enumerate}
	\item range finder
	\item driver point of view (POV) image
	\item displacement (distance from road centre to car centre)
	\item angle (rotation angle between car and road)
\end{enumerate}
Then, a deep neural network will be trained on the ground truth data to infer the angle and displacement from the provided POV image. Subsequently, a controller based on spiking neural networks (SNN) is trained to generate driving commands based on range finder, displacement and angle sensor inputs. Finally, the controller and the DNN are connected. So the overall model works only based on the POV image and the range finder data, which are directly taken from TORCS.

%As a competing group is working on the same task, a race will be carried out at the end and the winner will be determined. Thus, the goal is to drive the car around the track as quickly as possible.

\section{State of the art}
\label{sc:sota}
\the\textwidth

\subsection{Using deep neural networks for regression}

\section{Deep neural network architecture choice}
The implementation of the deep neural network was done in \texttt{python} version 3.6.3. To facilitate development, the library \texttt{keras} TODO: CITE KERAS SOURCE was used with the backend \texttt{tensorflow} TODO: CITE TENSORFLOW. To manipulate image data \texttt{openCV3} TODO: CITE OPENCV was used and finally to load and manipulate data the library \texttt{numpy} TODO: CITE NUMPY was utilised. 
\subsection{The testing deep neural network}
\label{ssc:testing-dnn}
To be able to chose hyperparametres for the convolutional neural network (CNN) at an early point, a basic CNN network architecture was chose which has been proven before. The criteria for chosing the network architecture was, that it has to provide reasonable performance while being quick to train. The focus was rather on good training performance, as the available resources are limited to us. Thus, after studying the architectures' training performances as seen here TODO: CITE TRAINING PERF GITHUB, AlexNet \cite{alexnet2012imagenet} was chosen.\\
AlexNet is a network designed for image classification tasks, such as the ImageNet challenge TODO: CITE IMAGENET CHALLENGE!. Therefore, the last layer of AlexNet was altered to use it for multidimensional regression problems such as guessing the angle and displacement from an image. To achieve that, the number of output neurons of the last fully connected layer (FCL) was reduced from 1000 to 2, as there are two numbers to guess. Furthermore, the last FCL used a rectified linear unit (ReLU) as activation function. 
\begin{equation}
	y = RELU
\end{equation}
To prevent the activation function from cropping the output to values greater than zero, a linear activation function was used: $ y = x $.


\subsection{Convolutional neural network}

\subsection{Input images}
The image data is acquired from TORCS \cite{TORCS} using ROS via the TORCS-ROS node \cite{mirus_torcs-ros_2017}. 

\subsubsection{Choosing a camera angle}
TORCS provides several camera perspectives: 
\begin{enumerate}
	\item Driver's view with hood
	\item Driver's view without hood
	\item Third person perspective: far
	\item Third person perspective: close
\end{enumerate}
All perspectives were evaluated based on the DNN described in \autoref{ssc:testing-dnn} 
and as final perspective TODO:XX was chosen. The basis for that decision can be seen in 
\autoref{fig:camera-evaluation}, which shows the mean absolute error for each view.
\begin{figure}
	\centering
	\fbox{\rule{0pt}{2cm} \rule{1.0\linewidth}{0pt}}
	\caption{Mean absolute error of the same DNN trained with multiple camera views}
	\label{fig:camera-evaluation}
\end{figure}

\subsubsection{Choosing the image size}
The images are provided by TORCS ROS \cite{mirus_torcs} at a rate of 10 frames per second (fps) at a resoultion of $ \SI{640}{\px} \times \SI{480}{\px} $. Because that image size is too large
to train a reasonably deep network in a reasonable time, the images are down-scaled prior to training as well as in the final application. To determine the image providing the best result, four different sizes were evaluated with the DNN described in \autoref{ssc:testing-dnn}: 
\begin{enumerate}
	\item $ \SI{320}{\px} \times \SI{240}{\px} $
	\item $ \SI{160}{\px} \times \SI{120}{\px} $
	\item $ \SI{80}{\px} \times \SI{60}{\px} $
	\item $ \SI{40}{\px} \times \SI{30}{\px} $
\end{enumerate}
First, the training images were collected from TORCS ROS at a resolution of $ \SI{640}{\px}\times \SI{480}{\px}$. Upong loading the images, \texttt{openCV} based downscaling was applied using the TODO: CHECK OPENCV SCALING ALGO algorithm. Second, the testing DNN was trained with the images at the mentioned resolutions and the mean absolute error 
for the validation set was recorded, which can be seen in \autoref{fig:img-res-evaluation}.
\begin{figure}
	\centering
	%\input{attachments/alexnet-val_mae-img_size_compare-09042-12547-22111.pgf}
	\caption{Mean absolute error of the same DNN trained with multiple image resolutions}
	\label{fig:img-res-evaluation}
\end{figure}

\subsection{Building a deep neural network with keras}
\label{ssc:keras}
Keras is a python library 
hello keras \cite{Toshev_2014_CVPR}

\section{Controller Architecture}
\label{ssc:controller}

The controller of the car is designed based on Spiking Neural Networks (SNN) which are implemented with nengo \cite{nengo}, a python framework for building large-scale neural systems that is based on the Neural Engineering Framework (NEF) \cite{nef}. \\
The controller is divided in several modular parts: acceleration / breaking, steering, gear changing, clutching. TODO: why modular approach? \\


\section{Experiments and evaluation}
\label{sc:evaluation}

\subsection{Mathematical equations an}
In case you have to typeset equations make sure that all equations are numbered.
The following example shows how to do so

\begin{equation}
	E = mc^2
\end{equation}

Sometimes you wish to align equations. This is possible with the (already
included) \texttt{align} package. The example in Equation \autoref{eq:lotkavolterra}
which gives the Lotka--Volterra, or predator-prey equations, shows how to use
it.

\begin{align}\label{eq:lotkavolterra}
	\frac{dx}{dt} &= \alpha x - \beta x y \\
	\frac{dy}{dt} &= \delta x y - \gamma y
\end{align}

\subsection{Figures, tables, algorithms}

Most reports need to include one of the mentioned objects. There are suitable
environments for each of them. If you are interested in typesetting algorithms,
have a look at the packages \texttt{algorithmic} and \texttt{algorithmx}. Make
sure that all algorithms are well formatted and clearly understandable!

Figures are included using the \texttt{figure} environment. All graphics should
be centered.  Please ensure that any point you wish to make is resolvable in a
printed copy of the paper. Resize fonts in figures to match the font in the
body text, and choose line widths which render effectively in print. An example
is shown in Figure \autoref{fig:onecolumnfigure}.
\texttt{includegraphics} will include the respective file.

\section{Conclusion}
\label{sc:conclusion}

%
% If you wish to put your work under a specific license, your free to do so. You
% are not obliged, so you can remove the following section if you wish to.
%
\section*{License}
\markright{LICENSE}
This work is licensed under the Creative Commons Attribution 3.0 Germany
License. To view a copy of this license,
visit \href{http://creativecommons.org/licenses/by/3.0/de/}{http://creativecommons.org} or send a letter
to Creative Commons, 171 Second Street, Suite 300, San
Francisco, California 94105, USA.


% each report should include all references that you cite in the work. Make sure
% that you include all references!
\bibliographystyle{ieee}
\bibliography{bibliography}

\end{document}
