%
% Example for a student report latex file. Adapt as necessary
%

% the following lines should stay as is
\documentclass[10pt,a4paper,twoside,journal]{IEEEtran}
\usepackage[nocompress]{cite}
\usepackage[pdftex]{graphicx}

% some packages that most people use or improve the result
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsbsy}
\usepackage{flushend}
% properly print units, enable compact product between units
\usepackage[inter-unit-product =\cdot]{siunitx}
% load units \bit, \byte usw
\sisetup{detect-weight=true, binary-units=true}
\DeclareSIUnit\px{px}
\usepackage{array}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{url}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{pgf}

\usepackage[switch]{lineno}
\makeatletter
\@ifpackageloaded{lineno}{%
	\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
	  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
	  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
	  \renewenvironment{#1}%
		 {\linenomath\csname old#1\endcsname}%
		 {\csname oldend#1\endcsname\endlinenomath}}%
	\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
	  \patchAmsMathEnvironmentForLineno{#1}%
	  \patchAmsMathEnvironmentForLineno{#1*}}%
	\AtBeginDocument{%
	\patchBothAmsMathEnvironmentsForLineno{equation}%
	\patchBothAmsMathEnvironmentsForLineno{align}%
	\patchBothAmsMathEnvironmentsForLineno{flalign}%
	\patchBothAmsMathEnvironmentsForLineno{alignat}%
	\patchBothAmsMathEnvironmentsForLineno{gather}%
	\patchBothAmsMathEnvironmentsForLineno{multline}%
	}}%
	{}
\makeatother

% add your additional packages here
\usepackage{lipsum}

\begin{document}

%
% When submitting the review, make sure to include the following line to enable
% line numbering. When submitting the final report, disable the following line!
%\linenumbers

%
% configure submission details
%

% here you can specify the day of submission
\newcommand{\submissiondate}{\today}

% please specify the type of your submission. E.g. Advanced Seminar or Practical
% Laboratory
\newcommand{\submissiontype}{Practical course: Computational Neuro Engineering}

% give information about when your course happened in form of SEMESTER YEAR,
% e.g. Winter Semester 2016. In addition, specify the "short version", e.g. WS
% 2016
\newcommand{\submissionterm}{Winter Semester 2017/2018}
\newcommand{\submissiontermshort}{WS 2017/2018}

% the full submission title
\newcommand{\submissiontitle}{Learning to drive based on multiple sensor cues
in The Open Racing Car Simulator (TORCS)}
% in case that you have a very long report title, make sure to provide a shorter
% version that can be used in the \markboth command further below. In case your
% topic is short, simply comment the next and uncommented the second line
\newcommand{\submissiontitleshort}{Autonomous driving in TORCS with multiple sensors}
%\newcommand{\submissiontitleshort}{\submissiontitle}

% author list. Make sure that you include your matriculation number in the
% section starting with \thanks. In addition, specify your supervisor(s).
\author{Tim~Bicker and Nimar~Blume%
	\thanks{\textbf{Authors}:
		Tim Bicker (03641870, tim.bicker@tum.de)
		and Nimar Blume (03638934, nimar.blume@tum.de)
		% the following includes additional information
		\textbf{Course}: \submissiontype{} \submissionterm{}
		\textbf{Submitted}: \submissiondate{}
		\textbf{Supervisor}: Florian Mirus.
		Neuroscientific System Theory (Prof. Dr. J\"org Conradt), Technische
		Universit\"at M\"unchen, Arcisstra√üe 21, 80333 M\"unchen, Germany.
}}

% both items should look alike and contain a short version of the type of work
% and semester (e.g. Advanced Seminar WS 2016, Project Laboratory SS 2017) and
% your report title. If your title is too long, find a shorter one.
\markboth{\submissiontype{} \submissiontermshort{}: \submissiontitleshort{}}
{\submissiontype{} \submissiontermshort{}: \submissiontitleshort{}}

% this generates the paper title
\title{\submissiontitle}
\maketitle

% write a short abstract to introduce the reader to your work
\begin{abstract}
	To implement an autonomous driver in The Open Racing Car Simulator (TORCS)
	we use a combination of a deep neural network (DNN) and a spiking neural network (SNN) based on multiple
	sensor cues. Specifically, the DNN predicts the current car displacement
	and angle relative to the road centre from a driver's view image. Based on the two values
	and an additional multi dimension range finder sensor a SNN generates driving commands for the car. Subsequently, the driving performance is evaluated on unseen tracks.
\end{abstract}

\begin{IEEEkeywords}
	deep learning, TORCS, convolutional neural network, spiking neural network, autonomous driving
\end{IEEEkeywords}

%
% Main body follows here. Only capitalize the first word in any title
%
\section{Introduction}
\label{sc:intro}

\IEEEPARstart{A}{utonomous} driving is currently a controversial subject, especially 
regarding feasibility and performance. To explore the details of autonomous driving, we built an abstract prototype focussing on the implementation of inferring location information from images and implementing a neural controller to drive a car. In this work we use The Open Racing Car Simulator (TORCS) to simulate a car on a race track and collect relevant sensor data. The goal is to use multiple sensor cues to drive the car safely around the defined test tracks. Further, we use the Robot Operating System (ROS) \cite{ros-2009} as an interface between TORCS and the controller \cite{mirus_torcs-ros_2017}. The following signals are used:
\begin{enumerate}
	\item range finder
	\item driver point of view (POV) image
	\item displacement (distance from road centre to car centre)
	\item angle (rotation angle between car and road)
\end{enumerate}
Then, a deep neural network will be trained on the ground truth data to infer the angle and displacement from the provided POV image. Subsequently, a controller based on spiking neural networks (SNN) is trained to generate driving commands based on range finder, displacement and angle sensor inputs. Finally, the controller and the DNN are connected. So the final model works only based on the POV image and the range finder data.

% As a competing group is working on the same task, a race will be carried out at the end and the winner will be determined. Thus, the goal is to drive the car around the track as quickly as possible while maintaining a safe driving style.

%\section{State of the art}
%\label{sc:sota}
%\the\textwidth
%\the\columnwidth

%\subsection{Using deep neural networks for regression}
%TODO: EXPLAIN current DNNs made for regression tasks

\section{A deep neural network for regression}
The implementation of the deep neural network is done in \texttt{python} version 3.6.3. To facilitate development, the library \texttt{keras} \cite{chollet2015keras} is used with the backend \texttt{tensorflow} \cite{tensorflow2015-whitepaper}. To manipulate and preprocess image data we use \texttt{openCV3} \cite{opencv_library} and finally to load and manipulate data the library \texttt{numpy} \cite{5725236} is utilised. 

\subsection{The testing deep neural network}
\label{ssc:testing-dnn}
To be able to chose hyperparametres for the convolutional neural network (CNN) at an early point, we chose a basic CNN network architecture which has been proven before. The criteria for choosing the network architecture was, that it has to provide reasonable prediction performance while being also being quick to train. The focus was rather on fast training performance, as the available resources are limited to us. Thus, after studying many architectures' training performances as seen here \cite{jjohnson-cnn-benchmarks}, the AlexNet \cite{alexnet2012imagenet} architecture was chosen as testing DNN, provinding good training performance with proven good prediction performance. \\

AlexNet is a network designed for image classification tasks, such as the ImageNet challenge. Therefore, we altered the last layer of AlexNet to use it for multidimensional regression problems such as inferring the angle and displacement from an input image. To achieve that, the number of output neurons of the last fully connected layer (FCL) was reduced from 1000 to 2, as there are two numbers to determine. Furthermore, the last FCL used a rectified linear unit (ReLU) as activation function. 
\begin{equation}
	f(x) = max(x, 0)
\end{equation}
To prevent the activation function from cropping the output to values greater than zero, a linear activation function is used instead: $ f(x) = x $. \\
Unless noted otherwise, we used the map \texttt{Olethros Road 1} with the testing network as training data. 

%\subsection{Convolutional neural network}
%TODO: DESCRIBE BASICS OF CNN

\subsection{Data set splitting}
\label{ssc:data-set-split}
TORCS provides 19 road tracks of which all are relevant. Therefore, data was recorded for all tracks. The recorded data set is split into three parts: 
\begin{enumerate}
	\item Training set
	\item Validataion set
	\item Test set
\end{enumerate}
First, the two tracks \texttt{Wheel 2} and \texttt{CG Track 2} were chosen to be used for testing. As the goal is to train a general model, a prerequisite is that data recorded on either of the two test tracks is not used during training. In total 6761 data points were recorded on the two test tracks. \\
The remaining tracks are used to train the DNN and to determine its hyperparametres. Therefore, the data set is randomly split into \texttt{train} and \texttt{val} (validataion) at a ratio of 90\% to 10\%. The validation set data itself was thus not used for traning, but it is recorded on tracks which are included in the training tracks. The test data set contains a total of 57180 entries while the validation set contains 6342 data points. \\
A data point consists of an input image (see \autoref{ssc:input-images}) with a corresponding distance and angle value, recorded at the same point in time.

\subsection{Input images}
\label{ssc:input-images}
The image data is acquired from TORCS \cite{TORCS} using ROS via the TORCS-ROS node \cite{mirus_torcs-ros_2017}. The image values are not normalised because all images have the same scale to begin with (0-255). The images are processed in the RGB colour space as this removes one more processing step. Furthermore, should a different colour space be better suited, for example HSV, then the DNN will learn the transformation itself as a colour space transformation is a simple linear transformation operation only. 

\subsubsection{Choosing a camera perspective}
TORCS provides four main car camera perspectives: 
\begin{enumerate}
	\item Driver's view with hood
	\item Driver's view without hood
	\item Third person perspective: far
	\item Third person perspective: close
\end{enumerate}
All perspectives are evaluated based on the test DNN described in \autoref{ssc:testing-dnn} 
and as final perspective the driver's view without hood is chosen. The basis for that decision can be seen in 
\autoref{fig:camera-evaluation}, which shows the mean absolute error for each view, with the driver's view without hood being the lowest.\\ 
The different perspectives are shown in \autoref{fig:car_views}.
\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{attachments/car_view_compilation.png}
	\caption{Different camera perspectives provided by TORCS}
	\label{fig:car_views}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{attachments/simple-val_mae-perspective_comp-30567-31159-31785-32414.pdf}
	\caption{Mean absolute error of the simple DNN trained with images from the camera perspectives shown in \autoref{fig:car_views}. 1st means first person while 3rd describes a 3rd person perspective.}
	\label{fig:camera-evaluation}
\end{figure}

\subsubsection{Choosing the image size}
The images are provided by TORCS-ROS \cite{mirus_torcs-ros_2017} at a rate of 10 frames per second (fps) at a resoultion of $ \SI{640}{\px} \times \SI{480}{\px} $. Because that image size is too large
to train a reasonably deep network in a reasonable time, the images are down-scaled prior to training as well as in the final application. To determine the image providing the best result, four different sizes were evaluated with the DNN described in \autoref{ssc:testing-dnn}: 
\begin{enumerate}
	\item $ \SI{320}{\px} \times \SI{240}{\px} $
	\item $ \SI{160}{\px} \times \SI{120}{\px} $
	\item $ \SI{80}{\px} \times \SI{60}{\px} $
	%\item $ \SI{40}{\px} \times \SI{30}{\px} $
\end{enumerate}
First, the training images were collected from TORCS-ROS at a resolution of $ \SI{640}{\px}\times \SI{480}{\px}$. Upon loading the images, \texttt{openCV} based downscaling was applied using the bilinear interpolation algorithm. Second, the testing DNN was trained with the images at the mentioned resolutions and the mean absolute error 
for the validation set was recorded, which is visualised in \autoref{fig:img-res-evaluation}.
\begin{figure}[ht]
	\centering
	%\input{attachments/alexnet-val_mae-img_size_compare-09042-12547-22111.pgf}
	\includegraphics[width=\columnwidth]{attachments/alexnet-val_mae-img_size_compare-09042-12547-22111.pdf}
	\caption{Mean absolute error of the same DNN trained with multiple image resolutions}
	\label{fig:img-res-evaluation}
\end{figure}

Therefore, the image size $ \SI{80}{\px}\times\SI{60}{\px} $ is used for the DNN as it provides the smallest mean absolute error and additionally reduces the training time due to the small image size. \\

Furthermore, we later discovered that the time the DNN needs to output the angle and distance prediction from an input image is also important to the controller. Therefore, a smaller image size is desired to reduce the prediction time. We did however not consider this in the initial design decisions. 

\subsection{Choosing the optimiser}
\label{ssc:optimiser-choice}
A plethora of different optimisers are available for use in the backpropagation step when training DNNs. The largest differences are in their abilities to exit large local maxima, training performance and accuracy. As the currently used DNN is managable in size, the training performance is not yet a critical property. Therefore, the optimiser was selected based on its ability to minimse the mean absolute error of the validation set. As illustrated in \autoref{fig:optimiser-evaluation}, the \texttt{adamax} optimiser provides the best result and is thus chosen for future application. \texttt{adamax} is similiar to the \texttt{adam} optimiser while being more stable in certain conditions, see \cite{DBLP:journals/corr/KingmaB14} for more details. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{attachments/alexnet-val_mae-optimiser_comp-69975-70890-71951-76023-79233-82251.pdf}
	\caption{Optimiser comparison in the test DNN using with an image size of $ \SI{80}{\px}\times\SI{60}{\px} $}
	\label{fig:optimiser-evaluation}
\end{figure}

\subsection{Choosing a DNN architecture}
\label{ssc:architecture-choice}
As a wide variety of DNN architectures are possible it was out of the scope of this course to develop and test a completely new architecture from scratch. Thus, we studied existing architectures and introduced modifications to mold the model towards regression tasks. \\
The network input is an image for which a Convolutional Neural Network (CNN) is well suited. As the CNN also functions as a feature extractor, manual feature extraction is not necessary. Multiple CNN layers are then followed by Fully Connected Layers (FCL), which are individual neurons each connected to one another. Finally, the last FCL has only two outputs which represent the two numbers to be inferred: displacement and angle. We tested various model architectures against each other and played around with adding more/less Convolutional or Fully Connected Layers. In the end, as shown in \autoref{fig:model-architecture-comparison} the error stayed more or less constant, irrespective of adding or removing layers. \\
The architecture of the simple model is shown in \autoref{fig:model-architecture}. \texttt{plu2d} referrs to two FCL being added to the model, while \texttt{min2d} indicates two FCL being removed. \texttt{invcnv} describes that the CNN layers were inverted, meaning that the first CNN layer has a small filter size (128) while the last one is larger (384). The adv model is different in that we added one more CNN layer with filter size 256.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{attachments/model-52797-rot.png}
	\caption{Structure of the simple DNN model}
	\label{fig:model-architecture}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{attachments/simple-val_mae-architecture_comp-02697-52797-65029-75770-89095.pdf}
	\caption{Comparison of the mean absolute error of different model architectures. Even though significant changes were introduced, the error did stay relatively constant.}
	\label{fig:model-architecture-comparison}
\end{figure}

\subsection{Building a deep neural network with keras}
\label{ssc:keras}
Keras \cite{Toshev_2014_CVPR} is a python library to simplify building Deep Neural Networks. It can use various backends to do the actual processing, of which we chose \texttt{tensorflow} \cite{tensorflow2015-whitepaper}. The process of building a DNN can be divided into six parts: overall process design, acquiring training/testing data, preprocessing training/testing data, building a DNN model architecture, training a DNN model and finally testing the model.

\section{Nengo Controller}
\label{sc:controller}
The controller of the car is designed based on Spiking Neural Networks (SNN) which are implemented in Nengo \cite{nengo}, a python framework for building large-scale neural systems that is based on the Neural Engineering Framework (NEF) \cite{nef}. \\
The controller is divided in several modular parts: acceleration, braking, steering, gear changing and clutching. We choose the modular approach, because it allows to test different modules independently from each other and also mix hard coded solutions with learned ones.

\subsection{Nengo controller design}
In the default driver's source code, the driving modules are based on different sensor inputs. The steering module is a function based on the car speed, the lateral displacement and the rotation angle. Both, the acceleration and the braking module are based on three range sensor signals with an angle of $-5$, $0$ and $5$ degree as well as the car speed. We chose to implement the gear changing as a hard coded solution without any learning involved, since it is a simple if-else condition. It is further possible to neglect the clutching module, because it did not show to make any difference for driver performance even though it is implemented in the default driver. The final model architecture, including the DNN, can be seen in \autoref{fig:nengo_architecture}.\\
\begin{figure}[ht]
	\centering
	%\input{attachments/alexnet-val_mae-img_size_compare-09042-12547-22111.pgf}
	\includegraphics[width=\columnwidth]{attachments/nengo_architecture.png}
	\caption{Nengo architecture with different modules. The input network contains an input node that has subscribed to the ROS topics and contains the CNN for inference of rotation angle and lateral displacement. The steer, accelerate and brake module contain one ensemble each. The output network contains an output node that publishes to the ROS topics. The gear module is not shown here.}
	\label{fig:nengo_architecture}
\end{figure}

\subsection{Controller Training}
In this work we chose a supervised learning approach for training the controller. Nengo offers two types of learning strategies: offline and online learning.
% Besides the functionality of Nengo to directly encode and decode hardcoded functions according to the NEF \cite{nef},
With Nengo deep learning \cite{nengo_dl}, an extension to implement classic deep learning methods exists as well.

\paragraph{offline learning} can be used for the classical supervised learning approach. Feature data and corresponding labels are given and Nengo uses a least squares approximation to solve for the neuron weights \cite{nef}.

\paragraph{online learning} iteratively improves the initial neuron configuration in a supervised manner during runtime. However, this requires that for every timestep during runtime a goal value has to be known for Nengo to solve for it. In our problem this is not the case. Therefore online learning is not further considered in this work.

\paragraph{Nengo deep learning} allows to train classical deep learning methods e.g. feedforward neural networks with backpropagation and then implement the learned nets into Nengo. \\
This work focuses on a mixture of hard coded functionality and offline learning.

\subsection{Evaluation metric}
To evaluate the controller's performance, we used the following two metrics: Firstly, we investigate if the controller is able to finish one lap in a reasonable time and how fast he is compared to the default driver. Secondly, we judge the overall robustness of the controller. For this we observed the controller during driving and evaluated how stable it drives the car, e.g. how close it drives to the edge of the street.

\subsection{Data Sources}
\paragraph{default TORCS-ROS driver}
The default TORCS-ROS driver \cite{mirus_torcs-ros_2017}. The advantage of this driver is that it drives a maximum speed of \SI{149}{\km\per\hour} and drives very carefully. Which means, that it always tries to stick to the middle lane of the road and brakes heavily when it comes close to a turn. We believe that this is an easy to learn and robust driving style. However, this driver is very slow compared to the other drivers (\autoref{tab:torcs_laptime}).
\paragraph{TORCS drivers}
Furthermore, there are several drivers that are implemented in TORCS. Those drivers have no speed limit and their driving style approximates that of a real race. The cars drive very close to the edge of the road and drive very fast through turns. We think that this is a hard to learn and error-prone driving style, because small errors lead to disastrous effects.
\paragraph{driving manually}
Finally, there is the option to drive manually. As it turns out, this is really difficult with a keyboard. Because fractions of seconds on the left and right arrows lead to very strong steering behavior and almost every time lead to a full turn-around or a far deviation of the track. This leads to a manual driving style that comes close to the default driver: very slow through turns and sticking close to the road center. However, because it is possible to drive as fast as one wants and in general one achieves a smoother driving of the car, we achieve a faster time than the default driver. \\

\begin{table}[ht]
	\centering
		\begin{tabular}{ |c|c|c| } 
			\hline
			TORCS-ROS default & 85s \\
			TORCS  & 43s  \\
			manual & 69s \\
			\hline
		\end{tabular}
		\caption{\label{tab:torcs_laptime}This table compares the lap times of different driver types.}
\end{table}
Because of the above mentioned characteristics of the different driver styles, it we decided to first learn the TORCS-ROS default driver, because it seems to be the most robust one and data generation is simple.

\section{Experiments and evaluation}
\label{sc:evaluation}

\subsection{Performance comparison between inferring angles vs. inferring displacement}
For the final application, the same model is used to infer the car's displacement and its angle. During training, the loss function (MSE) and the metrics (MAE) are continually evaluating the consolidated training performance. However, to determine the performance of predicting the individual values, two models were trained on each either predicting the angle or predicting the displacement of the car. \autoref{fig:mae-distance-vs-angle} shows the difference between angle and displacement prediction is shown. In training and validation set, the value of the displacement is in the range of -0.9 to 1.1 and the angle is in the range of -0.9 to 1.1. Thus, it may seem like the displacement value is more volatile, but the problem here is different. The value for the angle stays constant no matter the input image and thus is unreliable. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{attachments/alexnet-val_mae-angle_dist_comp-05425-86589.pdf}
	%\input{attachments/alexnet-val_mae-distance_angle_error-84095-86589-98073_-.pgf}
	\caption{Mean absolute error of two test networks trained only to predict either the displacement or angle of the car. The DNN learned the average angle over all data points and thus outputs a constant value for it.}
	\label{fig:mae-distance-vs-angle}
\end{figure}

\subsection{Prediction performance of the DNN}
The prediction performance indicates the time in ms which the DNN needs for one prediction given a single input image. Tested on an Intel Core i7-2650M the time fluctuates but stays within \SI{50}{\milli\second} and \SI{200}{\milli\second}. While that time is insufficient for the controller, the prediction can be carried out on a CUDA GPU which should result in a significant speed-up of the operation.

\subsection{Generalisation performance of the DNN}
\label{ssc:generalisation}
The generalisation evaluates the prediction performance of the DNN on the test set, so on previously unseen tracks. When trained on the whole training set as outlined in \autoref{ssc:data-set-split} prediction of the displacement from images on the test track result in a mean absolute error (MAE) of 0.2. While that value is not good, because it reflects a too drastic deviation from the real car position, the real problem in our case is the angle. Predicting the angle will result in an identical output value irrespective of the input image feeded into the DNN. We did try to lower the learning rate, increase or decrease the DNN depth, add regularisation and dropout layers but the angle did stay constant, which means that the DNN considers the most exact value to learn the average. This might be caused by the training data, in which a large amount of images correspond to only very small angle variations. However, even training with a smaller subset of the training set with percieved more angle variation did not result in an improvement. 

\subsection{Controller performance}
Controller performance is influenced by different factors, which are discussed in the following. \\
%For finetuning the results we have basically three tuning possibilities: the neuron sizes, the amount of data and the composition of tracks the data was collected from. \
In order to define the best model capacity, we looked at the generelization performance. The final model should not overfit nor underfit. Besides the neuron sizes for the different modules and signal encodings, also neuron composition is of interest. It is possible that one module has a more complex function and therefore needs a higher capacity to learn it's corresponding function. During testing, however, we did not see different module capacities to have a strong influence on the model. We increased the neuron sizes from a bad performing configuration until we hardly saw any improvements on the generalisation task. We could not reach overfitting with the dataset we used, because our hardware would eventually run out of physical memory.\\
When we collected track data for training the controller, we chose tracks with different shapes and turns in order to cover a vide variety of situations. We observed that small training sets would lead to overfitting of the model and for small model capacities to instable performance. Once we had enough data, the track composition seemd to hardly make a difference for controller performance. \\
% Further we observed that braking needed much more neurons than accelerating and steering to work reliable. We assume the reason for this is that the braking function in the driver is highly non linear and consists also of a ABS implementation. \\
% While testing we observed that even though lower neuron counts perform equally well in lap time, they, however, performe much worse in stability. E.g. a setup with  \\
% The controller performance based on ground truth data is shown in \autoref{tab:controller_laptime}.  \\
During the test phase of the controller performance we observed a large variance in the results. When we ran tracks multiple times with the same configuration, we obtained different track times with up to \SI{3}{\second} difference. Further, the same models showed slightly different behaviour in critical turns. We have not yet discovered the reason for this, but we believe it has something to do with the controller's control frequency, which is also influenced by the kernel management and other processes running on the system. Because our setup was Ubuntu 16.04 on a virtual machine, it is possible that a native host and therefore a more powerful system would show less variance. \\
% We could not test larger models, because our system ran out of physical memory in the building process (7,2 GB RAM).
% 2,800 / 3,400 - XX - xx ms -> instable, bigger laptime -> overfitting! However our model ran out of physical memory by increasing the data
% 2,400 / 3,000 / 4,500 - 2:02:91 - 1.6 ms
% 2,400 / 3,000 / 3,000 - 1:59:14 - 1.5 ms
% 2,400 / 3,000 / 3,000 - 1:59:90 - 2.6ms -> rausgehauen bei kurve
% 2:00:67 - 1.67 ms -> nicht rausgehauen
% 2:03:41 - 1.9ms
% mit 10ms signal delay 2:16:x-> 4ms prop time
% mit 10ms interrupt 2:01:59 -> 3,6 ms prop time
% mit 10ms lenkung interrupt 2:06:04 -> 3,6 ms prop time
%
% 2,000 / 2,400 / 2,400 - 2:00:41 | 0 - 1.5ms
% 2,000 / 2,400 / 2,400 - 2:01:59 | 0 - 1.47 ms
% mit 10 ms lenkung intterupt 2:04:33 -> 2,6 ms prop time
%
% 1,400 / 1,600 / 1,600 - 2:07:45 | 0.8 ms - bad performance
As already mentioned it is important to keep the control frequency high. This is necessary because with increasing model capacity, the calculation time of the controller becomes too large which lead to an instable controller. Low frequencies lead to the controller signals being already outdatet when they arrive at the car and therefore controlling becomes instable. This is especially important on computers with older hardware, which, even with native operating systems, have problems to simulate TORCS, ROS and Nengo at the same time. We observed that for average controller iteration times larger than \SI{12}{\milli\second} the controller already becomes instable. One forward propagation of the Nengo controller without a DNN takes approximately between \SI{1.5}{\milli\second} and \SI{3}{\milli\second}. This requires the DNN to have a maximum forward propagation of \SI{9}{\milli\second}. \\
Another influence on controller perfomance is the way how the signal is acquired. The forward propagation of the DNN in the input node (see \autoref{fig:nengo_architecture}) leads to a delay until the calculated signal enters the SNN. As stated above, only the steering module is based on the DNN signals. In order to gain the best controller performance, we updated the range finder signals and the speed signals asynchronously. So even when the DNN was calculating the angle and displacement values, the Nengo input node still recieved updates from the ROS topics. Then, after the DNN had calculated the signals, we fed the now outdated angle and displacement values together with the most recent speed and range finder signals into the SNN. \\
As the final neuron sizes for the different modules we chose $2,000$ neurons encoding each input signal. The driving modules consist of $2,400$ neurons each. With smaller model capacities we obtained a less stable driver, while a larger model capacity did not show any improvements. \autoref{tab:controller_laptime} shows the results of the controller without the DNN, with and without \SI{9}{\milli\second} artificial interruption with asynchronous signal updates based on ground truth data input signals. \\
\begin{table}[ht]
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			track &  default driver & no interruption & interruption \\
			\hline
			cg track 2  & 1:58:15 & 2:00:76 & 2:03:84  \\
			wheel 2 & 4:21:45 & 4:26:80 & 4:46:51 \\
			\hline
		\end{tabular}
		\caption{\label{tab:controller_laptime}This table compares the lap times of the controller with and without \SI{9}{\milli\second} artificial interruption against the default driver on the generalization task with ground truth data as input signals.}
	\end{center}
\end{table}
To elaborate the effects of noisy data input, we modified the input data with an additional gaussian noise with mean 0 and a standard deviation of 30\% of the signal value. We tested the model without an artificial interrupt and achieved a lap time of 2:04:14. With an artificial interrup of \SI{9}{\milli\second} we achieved a laptime of 2:30:31. Furthermore, due to the noisy data and the artificial delay the car drives in slaloms over the whole track. On the one hand this leads to a slower overall speed and therefore a faster lap time, but on the other hand the slower speed helps the car to drive in a stable through critical turns without crossing the edge of the street.
%noisy data:
% std deviation 0.3 -> 2:04:14
% std deviation 0.3 & 9ms interrupt -> 2:30:31 -> big slalom makes driver more stable in critical turns because of lower overall velocity

\subsection{Controller and DNN}
When we tested the controller and the DNN they failed to work together. The controller drove the car directly against the wall. One main problem is the too long propagation time from image to displacement and angle values, which makes the controller instable. Another problem lies in the unreliable inference values of the DNN, particularily the constant angle output as described in \autoref{ssc:generalisation}.

\section{Conclusion}
\label{sc:conclusion}
We identified critical factors for the controller performance and could also provide quantitative guidelines for controller stablity. A very important factor is the propagation time. A overall propagation time of less than \SI{12}{\milli\second} is desirable and puts an upper limit of approximately \SI{9}{\milli\second} on the capacity of the DNN. We further discovered that it is possible to drive the car stable, but slow, with a \SI{9}{\milli\second} propagation delay and 30\% standard deviation around the signal value. This points out a clear focus for the design of the DNN: it is more desirable to have a fast and less accurate DNN than a slow and accurate one. \\
As we realised the propagation time limit late during our research, we did not manage to successfully mitigagte the issue and speed up the propagation time. We were able to infer the car displacement from an input image with a mean absolute error of 0.2 but unable to reliably infer the angle of the car, which resulted in a constant output of the average angle. To mitigate this problem, we tried various methods as outlined in \autoref{ssc:generalisation} and further methods as discussed in \autoref{ssc:outlook} could be tried.

\subsection{Outlook}
\label{ssc:outlook}
%Change training data/camera view/angle etc to improve angle prediction performance. \\
It is possible to further enhance controller performance. The biggest room for improvement is a different default driver. A slow overall speed helps immensiley to drive the car in a stable manner through the track. Furthermore, steering, accelerating and breaking also proivde room for improvement. For example the driver could use more angles of the range finder sensor to be able to drive faster through turns. Another improvement possibility is to merge the accelerating and braking module into one module, so the controller can only either accelerate or brake. Nengo Deep Learning is an interesting approach to see if controller performance could be improved further. Also a quantitative measurement of controller stability could help in finding the optimal controller capacity. \\
We further need to investigate if it is possible to train a DNN that fulfills the recently discovered requirements. \\
To improve the performance of the DNN in inferring the displacement and angle of the car, slight manual feature extraction could be performed on the images. For example, the sky does not seem to provide any substantial information from which the displacement nor angle can be predicted and thus removing the top third of the image could improve the speed and accuracy during prediction and training. Furthermore, during the preprocessing it could make sense to perform mean image substraction on each image so that the image's values are centred around 0. Last, more diverse training data with large value deviations from driving at the centre of the road might improve angle and displacement prediction accuracy and could allow the car to also recover from driving faults, as the displacement of the car could be inferred even off track. 

% each report should include all references that you cite in the work. Make sure
% that you include all references!
\bibliographystyle{ieee}
\bibliography{bibliography}

\end{document}
